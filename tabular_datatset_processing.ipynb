{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Formatting ---\n",
      "Loading structured data from: output/structured_protein_data.pkl\n",
      "Loading data splits from: output/data_splits.json\n",
      "\n",
      "Subsetting the data to use 25.0% of proteins (stratified).\n",
      "Found 5071 proteins with epitopes and 2 without.\n",
      "Selected 1267 proteins with epitopes and 0 without.\n",
      "Total proteins to use: 1267 out of 5073.\n",
      "\n",
      "Reconstructing flat data arrays from the list of proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 1267/1267 [00:00<00:00, 93988.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset reconstructed. Total residues (samples): 475294, Features: 24\n",
      "Applying train/validation/test splits...\n",
      "\n",
      "Split sizes:\n",
      "  - Train: 198906 residues from 600 proteins. Shape: (198906, 24)\n",
      "  - Validation: 26865 residues from 80 proteins. Shape: (26865, 24)\n",
      "  - Test: 55533 residues from 180 proteins. Shape: (55533, 24)\n",
      "\n",
      "Saving formatted data to directory: 'output/epitope_prediction'\n",
      "Metadata saved to: output/epitope_prediction/info.json\n",
      "\n",
      "--- Formatting Complete! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# format_dataset.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import config to get paths to the source files\n",
    "import config\n",
    "\n",
    "def format_epitope_dataset():\n",
    "    \"\"\"\n",
    "    Reads the structured .pkl dataset and the pre-computed splits,\n",
    "    and reformats them into a directory containing separate .npy files for\n",
    "    train, validation, and test sets, along with a metadata info.json file.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # The name of the output directory for the newly formatted dataset\n",
    "    DATASET_NAME = 'epitope_prediction'\n",
    "    TARGET_DIR = os.path.join('output', DATASET_NAME)\n",
    "    DATA_PERCENTAGE_TO_USE = 1.0\n",
    "\n",
    "    print(\"--- Starting Dataset Formatting ---\")\n",
    "\n",
    "    # --- 1. Check for source files ---\n",
    "    if not os.path.exists(config.STRUCTURED_DATA_PATH):\n",
    "        print(f\"Error: Source file not found at '{config.STRUCTURED_DATA_PATH}'\")\n",
    "        print(\"Please run the feature engineering pipeline first.\")\n",
    "        return\n",
    "    if not os.path.exists(config.SPLITS_FILE_PATH):\n",
    "        print(f\"Error: Splits file not found at '{config.SPLITS_FILE_PATH}'\")\n",
    "        print(\"Please run the sequence clustering step in the pipeline first.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Load the source data and splits ---\n",
    "    print(f\"Loading structured data from: {config.STRUCTURED_DATA_PATH}\")\n",
    "    with open(config.STRUCTURED_DATA_PATH, 'rb') as f:\n",
    "        protein_data_list = pickle.load(f)\n",
    "\n",
    "    print(f\"Loading data splits from: {config.SPLITS_FILE_PATH}\")\n",
    "    with open(config.SPLITS_FILE_PATH, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "        \n",
    "    if DATA_PERCENTAGE_TO_USE < 1.0:\n",
    "        print(f\"\\nSubsetting the data to use {DATA_PERCENTAGE_TO_USE * 100}% of proteins (stratified).\")\n",
    "        proteins_with_positives = []\n",
    "        proteins_without_positives = []\n",
    "        for p in protein_data_list:\n",
    "            if np.any(p['df_stats']['is_epitope'].values):\n",
    "                proteins_with_positives.append(p['pdb_id'])\n",
    "            else:\n",
    "                proteins_without_positives.append(p['pdb_id'])\n",
    "        print(f\"Found {len(proteins_with_positives)} proteins with epitopes and {len(proteins_without_positives)} without.\")\n",
    "        np.random.shuffle(proteins_with_positives)\n",
    "        np.random.shuffle(proteins_without_positives)\n",
    "        num_pos_to_take = int(len(proteins_with_positives) * DATA_PERCENTAGE_TO_USE)\n",
    "        num_neg_only_to_take = int(len(proteins_without_positives) * DATA_PERCENTAGE_TO_USE)\n",
    "        if len(proteins_with_positives) > 0 and num_pos_to_take == 0:\n",
    "            num_pos_to_take = 1\n",
    "        selected_pos_proteins = proteins_with_positives[:num_pos_to_take]\n",
    "        selected_neg_only_proteins = proteins_without_positives[:num_neg_only_to_take]\n",
    "        selected_protein_ids = set(selected_pos_proteins + selected_neg_only_proteins)\n",
    "        print(f\"Selected {len(selected_pos_proteins)} proteins with epitopes and {len(selected_neg_only_proteins)} without.\")\n",
    "        print(f\"Total proteins to use: {len(selected_protein_ids)} out of {len(protein_data_list)}.\")\n",
    "        protein_data_list = [p for p in protein_data_list if p['pdb_id'] in selected_protein_ids]\n",
    "\n",
    "    # --- 4. Reconstruct the full, flat dataset arrays ---\n",
    "    print(\"\\nReconstructing flat data arrays from the list of proteins...\")\n",
    "    features, labels, groups = [], [], []\n",
    "    for protein_data in tqdm(protein_data_list, desc=\"Processing proteins\"):\n",
    "        features.append(protein_data['X_arr'])\n",
    "        labels.append(protein_data['df_stats']['is_epitope'].values)\n",
    "        groups.append(np.full(protein_data['length'], protein_data['pdb_id']))\n",
    "        \n",
    "    if not features:\n",
    "        print(\"Error: No data left after subsetting. Check your percentage or source data.\")\n",
    "        return\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    y = np.concatenate(labels)\n",
    "    groups = np.concatenate(groups)\n",
    "    print(f\"Full dataset reconstructed. Total residues (samples): {len(y)}, Features: {X.shape[1]}\")\n",
    "\n",
    "    # --- 5. Partition the data based on the pre-computed splits ---\n",
    "    print(\"Applying train/validation/test splits...\")\n",
    "    train_groups = splits['train']\n",
    "    val_groups = splits['val']\n",
    "    test_groups = splits['test']\n",
    "\n",
    "    # Create boolean masks to select rows for each split\n",
    "    # The masks will only be true for proteins in the selected subset\n",
    "    train_mask = np.isin(groups, train_groups)\n",
    "    val_mask = np.isin(groups, val_groups)\n",
    "    test_mask = np.isin(groups, test_groups)\n",
    "\n",
    "    # Apply masks to get the final data arrays\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    print(\"\\nSplit sizes:\")\n",
    "    print(f\"  - Train: {len(y_train)} residues from {len(np.unique(groups[train_mask]))} proteins. Shape: {X_train.shape}\")\n",
    "    print(f\"  - Validation: {len(y_val)} residues from {len(np.unique(groups[val_mask]))} proteins. Shape: {X_val.shape}\")\n",
    "    print(f\"  - Test: {len(y_test)} residues from {len(np.unique(groups[test_mask]))} proteins. Shape: {X_test.shape}\")\n",
    "\n",
    "    # --- 6. Save the data into the target directory structure ---\n",
    "    print(f\"\\nSaving formatted data to directory: '{TARGET_DIR}'\")\n",
    "    os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "    # In your case, all features are numerical, so we use the 'X_num' prefix\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_train.npy'), X_train.astype(np.float32))\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_val.npy'), X_val.astype(np.float32))\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_test.npy'), X_test.astype(np.float32))\n",
    "\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_val.npy'), y_val)\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_test.npy'), y_test)\n",
    "\n",
    "    # --- 7. Create and save the info.json metadata file ---\n",
    "    info = {\n",
    "        'name': DATASET_NAME,\n",
    "        'task_type': 'binclass',  # This is a binary classification task\n",
    "        'train_size': len(y_train),\n",
    "        'val_size': len(y_val),\n",
    "        'test_size': len(y_test),\n",
    "        'n_num_features': X_train.shape[1],\n",
    "        'n_cat_features': 0,  # You have no categorical features in this model\n",
    "    }\n",
    "\n",
    "    info_path = os.path.join(TARGET_DIR, 'info.json')\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(info, f, indent=4)\n",
    "    print(f\"Metadata saved to: {info_path}\")\n",
    "\n",
    "    print(\"\\n--- Formatting Complete! ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    format_epitope_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
