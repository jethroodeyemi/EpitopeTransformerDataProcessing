{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1d6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Formatting ---\n",
      "Loading structured data from: output/test_structured_protein_data.pkl\n",
      "Loading data splits from: output/data_splits.json\n",
      "Reconstructing flat data arrays from the list of proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 5073/5073 [00:00<00:00, 23754.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset reconstructed. Total residues (samples): 1827205, Features: 23\n",
      "Applying train/validation/test splits...\n",
      "\n",
      "Split sizes:\n",
      "  - Train: 812152 residues from 2560 proteins. Shape: (812152, 23)\n",
      "  - Validation: 100929 residues from 306 proteins. Shape: (100929, 23)\n",
      "  - Test: 203754 residues from 688 proteins. Shape: (203754, 23)\n",
      "\n",
      "Saving formatted data to directory: 'output/epitope_prediction'\n",
      "Metadata saved to: output/epitope_prediction/info.json\n",
      "\n",
      "--- Formatting Complete! ---\n"
     ]
    }
   ],
   "source": [
    "# format_dataset.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import config to get paths to the source files\n",
    "import config\n",
    "\n",
    "def format_epitope_dataset():\n",
    "    \"\"\"\n",
    "    Reads the structured .pkl dataset and the pre-computed splits,\n",
    "    and reformats them into a directory containing separate .npy files for\n",
    "    train, validation, and test sets, along with a metadata info.json file.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # The name of the output directory for the newly formatted dataset\n",
    "    DATASET_NAME = 'epitope_prediction'\n",
    "    TARGET_DIR = os.path.join('output', DATASET_NAME)\n",
    "    # ---------------------\n",
    "\n",
    "    print(\"--- Starting Dataset Formatting ---\")\n",
    "\n",
    "    # --- 1. Check for source files ---\n",
    "    if not os.path.exists(config.STRUCTURED_DATA_PATH):\n",
    "        print(f\"Error: Source file not found at '{config.STRUCTURED_DATA_PATH}'\")\n",
    "        print(\"Please run the feature engineering pipeline first.\")\n",
    "        return\n",
    "    if not os.path.exists(config.SPLITS_FILE_PATH):\n",
    "        print(f\"Error: Splits file not found at '{config.SPLITS_FILE_PATH}'\")\n",
    "        print(\"Please run the sequence clustering step in the pipeline first.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Load the source data and splits ---\n",
    "    print(f\"Loading structured data from: {config.STRUCTURED_DATA_PATH}\")\n",
    "    with open(config.STRUCTURED_DATA_PATH, 'rb') as f:\n",
    "        protein_data_list = pickle.load(f)\n",
    "\n",
    "    print(f\"Loading data splits from: {config.SPLITS_FILE_PATH}\")\n",
    "    with open(config.SPLITS_FILE_PATH, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "\n",
    "    # --- 3. Reconstruct the full, flat dataset arrays ---\n",
    "    print(\"Reconstructing flat data arrays from the list of proteins...\")\n",
    "    features, labels, groups = [], [], []\n",
    "    for protein_data in tqdm(protein_data_list, desc=\"Processing proteins\"):\n",
    "        features.append(protein_data['X_arr'])\n",
    "        labels.append(protein_data['df_stats']['is_epitope'].values)\n",
    "        groups.append(np.full(protein_data['length'], protein_data['pdb_id']))\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    y = np.concatenate(labels)\n",
    "    groups = np.concatenate(groups)\n",
    "    print(f\"Full dataset reconstructed. Total residues (samples): {len(y)}, Features: {X.shape[1]}\")\n",
    "\n",
    "    # --- 4. Partition the data based on the pre-computed splits ---\n",
    "    print(\"Applying train/validation/test splits...\")\n",
    "    train_groups = splits['train']\n",
    "    val_groups = splits['val']\n",
    "    test_groups = splits['test']\n",
    "\n",
    "    # Create boolean masks to select rows for each split\n",
    "    train_mask = np.isin(groups, train_groups)\n",
    "    val_mask = np.isin(groups, val_groups)\n",
    "    test_mask = np.isin(groups, test_groups)\n",
    "\n",
    "    # Apply masks to get the final data arrays\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    print(\"\\nSplit sizes:\")\n",
    "    print(f\"  - Train: {len(y_train)} residues from {len(train_groups)} proteins. Shape: {X_train.shape}\")\n",
    "    print(f\"  - Validation: {len(y_val)} residues from {len(val_groups)} proteins. Shape: {X_val.shape}\")\n",
    "    print(f\"  - Test: {len(y_test)} residues from {len(test_groups)} proteins. Shape: {X_test.shape}\")\n",
    "\n",
    "    # --- 5. Save the data into the target directory structure ---\n",
    "    print(f\"\\nSaving formatted data to directory: '{TARGET_DIR}'\")\n",
    "    os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "    # In your case, all features are numerical, so we use the 'X_num' prefix\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_train.npy'), X_train.astype(np.float32))\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_val.npy'), X_val.astype(np.float32))\n",
    "    np.save(os.path.join(TARGET_DIR, 'X_num_test.npy'), X_test.astype(np.float32))\n",
    "\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_val.npy'), y_val)\n",
    "    np.save(os.path.join(TARGET_DIR, 'y_test.npy'), y_test)\n",
    "\n",
    "    # --- 6. Create and save the info.json metadata file ---\n",
    "    info = {\n",
    "        'name': DATASET_NAME,\n",
    "        'task_type': 'binclass',  # This is a binary classification task\n",
    "        'train_size': len(y_train),\n",
    "        'val_size': len(y_val),\n",
    "        'test_size': len(y_test),\n",
    "        'n_num_features': X_train.shape[1],\n",
    "        'n_cat_features': 0,  # You have no categorical features in this model\n",
    "    }\n",
    "\n",
    "    info_path = os.path.join(TARGET_DIR, 'info.json')\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(info, f, indent=4)\n",
    "    print(f\"Metadata saved to: {info_path}\")\n",
    "\n",
    "    print(\"\\n--- Formatting Complete! ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    format_epitope_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
